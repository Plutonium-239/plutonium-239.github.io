<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>`memsave_torch` | pu239's page</title>
<meta name=keywords content="optimization,memory efficiency,fine tuning,CNNs"><meta name=description content="Lowering PyTorch's Memory Consumption for Selective Differentiation"><meta name=author content><link rel=canonical href=https://plutonium-239.github.io/projects/memsave_torch/><meta name=google-site-verification content="m8D5EaRXMmbmTIrarfEuOk3y54T12n_B7OD5D9k-fR4"><link crossorigin=anonymous href=/assets/css/stylesheet.8afda564a37916d73595609479ad9497581eda46cd27f7872ab630886ce7f9ed.css integrity="sha256-iv2lZKN5Ftc1lWCUea2Ul1ge2kbNJ/eHKrYwiGzn+e0=" rel="preload stylesheet" as=style><link rel=icon href=https://plutonium-239.github.io/bmth+av_round.png><link rel=icon type=image/png sizes=16x16 href=https://plutonium-239.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://plutonium-239.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://plutonium-239.github.io/apple-touch-icon.png><link rel=mask-icon href=https://plutonium-239.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://plutonium-239.github.io/projects/memsave_torch/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style type=text/css>:root{--accentColor:#00e1d2}</style><meta property="og:title" content="`memsave_torch`"><meta property="og:description" content="Lowering PyTorch's Memory Consumption for Selective Differentiation"><meta property="og:type" content="article"><meta property="og:url" content="https://plutonium-239.github.io/projects/memsave_torch/"><meta property="og:image" content="https://plutonium-239.github.io/projects/memsave_torch/images/memsave_torch_banner.svg"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-18T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://plutonium-239.github.io/projects/memsave_torch/images/memsave_torch_banner.svg"><meta name=twitter:title content="`memsave_torch`"><meta name=twitter:description content="Lowering PyTorch's Memory Consumption for Selective Differentiation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://plutonium-239.github.io/projects/"},{"@type":"ListItem","position":2,"name":"`memsave_torch`","item":"https://plutonium-239.github.io/projects/memsave_torch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"`memsave_torch`","name":"\u0060memsave_torch\u0060","description":"Lowering PyTorch's Memory Consumption for Selective Differentiation","keywords":["optimization","memory efficiency","fine tuning","CNNs"],"articleBody":" plutonium-239/memsave_torch Paper (WANT@ICML’24) arXiv:2404.12406 Poster Memory is a limiting resource for many deep learning tasks. Beside the neural network weights, one main memory consumer is the computation graph built up by automatic differentiation (AD) for backpropagation. We observe that PyTorch’s current AD implementation sometimes neglects information about parameter differentiability when storing the computation graph. This information is useful though to reduce memory whenever gradients are requested for a parameter subset, as is the case in many modern fine-tuning tasks. Specifically, inputs to layers that act linearly in their parameters and inputs (fully-connected, convolution, or batch normalization layers in evaluation mode) can be discarded whenever the parameters are marked as non-differentiable. We provide a drop-in, differentiability-agnostic implementation of such layers and demonstrate its ability to reduce memory without affecting run time on popular convolution- and attention-based architectures.\nSaving Memory in CNNs (with PyTorch) CNNs are made of mainly Convolution layers along with activations such as ReLU and normalization/pooling layers such as batch normalization and max pooling.\nMotivation In torch these layers are implemented by torch.nn.Conv2d, torch.nn.ReLU, torch.nn.BatchNorm2d,torch.nn.MaxPool2d, and give rise to very fast code that calls native C++ and CUDA kernels at the lowest level. This holds for the forward pass as well as the backward pass. However, sometimes memory efficiency is traded off for better time efficiency by storing a lot of tensors that we might need. But this results in consumer-level GPUs to not have enough VRAM for performing these tasks with a decent batch size. Here, we try to make that possible by implementing our own memory saving layers while also not giving up time efficiency. These can be found in the memsave module.\n==An important use case is when you want to slightly alter a layer==\nResults Here are summarized results on 4 models (plots from the poster):\nResNet-101 EfficientNet-v2-L T5 Mistral-7B ","wordCount":"305","inLanguage":"en","image":"https://plutonium-239.github.io/projects/memsave_torch/images/memsave_torch_banner.svg","datePublished":"2024-06-18T00:00:00Z","dateModified":"2024-06-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://plutonium-239.github.io/projects/memsave_torch/"},"publisher":{"@type":"Organization","name":"pu239's page","logo":{"@type":"ImageObject","url":"https://plutonium-239.github.io/bmth+av_round.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://plutonium-239.github.io/ accesskey=h title="pu239's page (Alt + H)"><img src=https://plutonium-239.github.io/bmth+av.png alt aria-label=logo height=40>pu239's page</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://plutonium-239.github.io/about/ title="about me"><span>about me</span></a></li><li><a href=https://plutonium-239.github.io/publications/ title=publications><span>publications</span></a></li><li><a href=https://plutonium-239.github.io/projects/ title=projects><span class=active>projects</span></a></li><li><a href=https://plutonium-239.github.io/other_stuff/ title="other stuff"><span>other stuff</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><span class=breadcrumbs-lang-badge><div class=breadcrumbs><a href=https://plutonium-239.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://plutonium-239.github.io/projects/></a></div></span><h1 class="post-title entry-hint-parent"><code>memsave_torch</code></h1><div class=post-description>Lowering PyTorch&rsquo;s Memory Consumption for Selective Differentiation</div><div class=post-meta><span title='2024-06-18 00:00:00 +0000 UTC'>June 18, 2024</span><ul class=post-tags><li><a href=https://plutonium-239.github.io/tags/optimization/>Optimization</a></li><li><a href=https://plutonium-239.github.io/tags/memory-efficiency/>Memory Efficiency</a></li><li><a href=https://plutonium-239.github.io/tags/fine-tuning/>Fine Tuning</a></li><li><a href=https://plutonium-239.github.io/tags/cnns/>CNNs</a></li></ul></div></header><figure class=entry-cover><img loading=eager class=lightbox src=https://plutonium-239.github.io/projects/memsave_torch/images/memsave_torch_banner.svg alt=Logo><p>Logo</p></figure><div class=post-content><p><span class=social-icons><a href=https://github.com/plutonium-239/memsave_torch target=_blank rel="noopener noreferrer me" title=Github data-text><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>plutonium-239/memsave_torch</a>
</span><span class=social-icons><a href="https://openreview.net/pdf?id=KsUUzxUK7N" target=_blank rel="noopener noreferrer me" title=Openreview data-text><svg viewBox="0 0 384 448"><path fill="currentcolor" d="M0 0v448h352V111.02L240.98.0zm32 32h192v96h96v288H32z"/></svg>Paper (WANT@ICML’24)</a>
</span><span class=social-icons><a href=https://arxiv.org/abs/2404.12406 target=_blank rel="noopener noreferrer me" title=Arxiv data-text><svg fill="currentcolor" viewBox="0 0 24 24"><path d="M3.8423.0a1.0037 1.0037.0 00-.922.6078c-.1536.3687-.0438.6275.2938 1.1113l6.9185 8.3597-1.0223 1.1058a1.0393 1.0393.0 00.003 1.4229l1.2292 1.3135-5.4391 6.4444c-.2803.299-.4538.823-.2971 1.1986a1.0253 1.0253.0 00.9585.635.9133.9133.0 00.6891-.3405l5.783-6.126 7.4902 8.0051a.8527.8527.0 00.6835.2597.9575.9575.0 00.8777-.6138c.1577-.377-.017-.7502-.306-1.1407l-7.0518-8.3418 1.0632-1.13a.9626.9626.0 00.0089-1.3165L4.6336.4639s-.3733-.4535-.768-.463zm0 .272h.0166c.2179.0052.4874.2715.5644.3639l.005.006.0052.0055 10.169 10.9905a.6915.6915.0 01-.0072.945l-1.0666 1.133-1.4982-1.7724-8.5994-10.39c-.3286-.472-.352-.6183-.2592-.841a.7307.7307.0 01.6704-.4401zm14.341 1.5701a.877.877.0 00-.6554.2418l-5.6962 6.1584 1.6944 1.8319 5.3089-6.5138c.3251-.4335.479-.6603.3247-1.0292a1.1205 1.1205.0 00-.9763-.689zm-7.6557 12.2823 1.3186 1.4135-5.7864 6.1295a.6494.6494.0 01-.4959.26.7516.7516.0 01-.706-.4669c-.1119-.2682.0359-.6864.2442-.9083l.0051-.0055.0047-.0055z"/></svg>arXiv:2404.12406</a>
</span><span class=social-icons align=center><a href=https://github.com/plutonium-239/memsave_torch/blob/main/memsave_poster.pdf target=_blank rel="noopener noreferrer me" title=Poster data-text><svg fill="currentcolor" viewBox="0 0 512 512" width="512" height="512"><path d="M473.938 308.314 409.913 33.017H102.084L38.061 308.314.0 471.967l30.174 7.017 32.488-139.691h251.421L283.228 471.968l30.174 7.017 32.488-139.691h103.45l32.488 139.691 30.174-7.017zm-152.651.0H69.865l56.82-244.318h251.421zm31.806.0 44.521-191.431 44.521 191.431z"/><path d="M149.448 94.195h181.087v30.978H149.448zm-10.667 50.98H319.869v30.978H138.781zm-10.677 50.98h181.087v30.978H128.104zm-10.667 50.98h181.087v30.978H117.438z"/></svg>Poster</a></span></p><p>Memory is a limiting resource for many deep learning tasks. Beside the neural network weights, one main memory consumer is the computation graph built up by automatic differentiation (AD) for backpropagation. We observe that PyTorch’s current AD implementation sometimes neglects information about parameter differentiability when storing the computation graph. This information is useful though to reduce memory whenever gradients are requested for a parameter subset, as is the case in many modern fine-tuning tasks. Specifically, inputs to layers that act linearly in their parameters and inputs (fully-connected, convolution, or batch normalization layers in evaluation mode) can be discarded whenever the parameters are marked as non-differentiable. We provide a <a href=https://github.com/plutonium-239/memsave_torch>drop-in, differentiability-agnostic implementation</a> of such layers and demonstrate its ability to reduce memory without affecting run time on popular convolution- and attention-based architectures.</p><h1 id=saving-memory-in-cnns-with-pytorch>Saving Memory in CNNs (with PyTorch)<a hidden class=anchor aria-hidden=true href=#saving-memory-in-cnns-with-pytorch>#</a></h1><p>CNNs are made of mainly Convolution layers along with activations such as ReLU and normalization/pooling layers such as batch normalization and max pooling.</p><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>In torch these layers are implemented by <code>torch.nn.Conv2d</code>, <code>torch.nn.ReLU</code>, <code>torch.nn.BatchNorm2d</code>,<code>torch.nn.MaxPool2d</code>, and give rise to very fast code that calls native C++ and CUDA kernels at the lowest level. This holds for the forward pass as well as the backward pass. However, sometimes memory efficiency is traded off for better time efficiency by storing a lot of tensors that we <em>might</em> need. But this results in consumer-level GPUs to not have enough VRAM for performing these tasks with a decent batch size. Here, we try to make that possible by implementing our own memory saving layers while also not giving up time efficiency. These can be found in the <code>memsave</code> module.</p><p>==An important use case is when you want to slightly alter a layer==</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>Here are summarized results on 4 models (plots from the poster):</p><h3 id=resnet-101>ResNet-101<a hidden class=anchor aria-hidden=true href=#resnet-101>#</a></h3><img loading=lazy class=lightbox src=images/poster_plot_resnet101.svg title=ResNet-101><h3 id=efficientnet-v2-l>EfficientNet-v2-L<a hidden class=anchor aria-hidden=true href=#efficientnet-v2-l>#</a></h3><img loading=lazy class=lightbox src=images/poster_plot_efficientnet_v2_l.svg title=EfficientNet-v2-L><h3 id=t5>T5<a hidden class=anchor aria-hidden=true href=#t5>#</a></h3><img loading=lazy class=lightbox src=images/poster_plot_t5.svg title=T5><h3 id=mistral-7b>Mistral-7B<a hidden class=anchor aria-hidden=true href=#mistral-7b>#</a></h3><img loading=lazy class=lightbox src=images/poster_plot_mistral-7b.svg title=Mistral-7B></div><footer class=post-footer><ul class=post-tags><li><a href=https://plutonium-239.github.io/tags/optimization/>Optimization</a></li><li><a href=https://plutonium-239.github.io/tags/memory-efficiency/>Memory Efficiency</a></li><li><a href=https://plutonium-239.github.io/tags/fine-tuning/>Fine Tuning</a></li><li><a href=https://plutonium-239.github.io/tags/cnns/>CNNs</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://plutonium-239.github.io/>pu239's page</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/lightbox.js></script><link rel=stylesheet href=/lightbox.css><script data-goatcounter=https://pu239.goatcounter.com/count>(function(){"use strict";window.goatcounter&&window.goatcounter.vars?window.goatcounter=window.goatcounter.vars:window.goatcounter=window.goatcounter||{};var e,t,n,o,i,a,r,c,l,d,u,s=document.querySelector("script[data-goatcounter]");if(s&&s.dataset.goatcounterSettings){try{o=JSON.parse(s.dataset.goatcounterSettings)}catch(e){console.error("invalid JSON in data-goatcounter-settings: "+e)}for(i in o)["no_onload","no_events","allow_local","allow_frame","path","title","referrer","event"].indexOf(i)>-1&&(window.goatcounter[i]=o[i])}e=encodeURIComponent,l=function(e){var s,o,i,t={p:e.path===void 0?goatcounter.path:e.path,r:e.referrer===void 0?goatcounter.referrer:e.referrer,t:e.title===void 0?goatcounter.title:e.title,e:!!(e.event||goatcounter.event),s:[window.screen.width,window.screen.height,window.devicePixelRatio||1],b:d(),q:location.search};return typeof t.r=="function"&&(s=t.r),typeof t.t=="function"&&(i=t.t),typeof t.p=="function"&&(o=t.p),n(t.r)&&(t.r=document.referrer),n(t.t)&&(t.t=document.title),n(t.p)&&(t.p=r()),s&&(t.r=s(t.r)),i&&(t.t=i(t.t)),o&&(t.p=o(t.p)),t},n=function(e){return e==null||typeof e=="function"},d=function(){var e=window,t=document;return e.callPhantom||e._phantom||e.phantom?150:e.__nightmare?151:t.__selenium_unwrapped||t.__webdriver_evaluate||t.__driver_evaluate?152:navigator.webdriver?153:0},u=function(t){var n,s=[];for(n in t)t[n]!==""&&t[n]!==null&&t[n]!==void 0&&t[n]!==!1&&s.push(e(n)+"="+e(t[n]));return"?"+s.join("&")},t=function(e){console&&"warn"in console&&console.warn("goatcounter: "+e)},a=function(){var e=document.querySelector("script[data-goatcounter]");return e&&e.dataset.goatcounter?e.dataset.goatcounter:goatcounter.endpoint||window.counter},r=function(){var e,t=location,n=document.querySelector('link[rel="canonical"][href]');return n&&(e=document.createElement("a"),e.href=n.href,e.hostname.replace(/^www\./,"")===location.hostname.replace(/^www\./,"")&&(t=e)),t.pathname+t.search||"/"},c=function(e){document.body===null?document.addEventListener("DOMContentLoaded",function(){e()},!1):e()},goatcounter.filter=function(){return"visibilityState"in document&&document.visibilityState==="prerender"?"visibilityState":!goatcounter.allow_frame&&location!==parent.location?"frame":!goatcounter.allow_local&&location.hostname.match(/(localhost$|^127\.|^10\.|^172\.(1[6-9]|2[0-9]|3[0-1])\.|^192\.168\.|^0\.0\.0\.0$)/)?"localhost":!goatcounter.allow_local&&location.protocol==="file:"?"localfile":!!(localStorage&&localStorage.getItem("skipgc")==="t")&&"disabled with #toggle-goatcounter"},window.goatcounter.url=function(e){var s,n=l(e||{});if(n.p===null)return;return n.rnd=Math.random().toString(36).substr(2,5),s=a(),s?s+u(n):t("no endpoint found")},window.goatcounter.count=function(e){var n,s,i,o=goatcounter.filter();if(o)return t("not counting because of: "+o);if(s=goatcounter.url(e),!s)return t("not counting because path callback returned null");navigator.sendBeacon(s)||(n=document.createElement("img"),n.src=s,n.style.position="absolute",n.style.bottom="0px",n.style.width="1px",n.style.height="1px",n.loading="eager",n.setAttribute("alt",""),n.setAttribute("aria-hidden","true"),i=function(){n&&n.parentNode&&n.parentNode.removeChild(n)},n.addEventListener("load",i,!1),document.body.appendChild(n))},window.goatcounter.get_query=function(e){for(var n=location.search.substr(1).split("&"),t=0;t<n.length;t++)if(n[t].toLowerCase().indexOf(e.toLowerCase()+"=")===0)return n[t].substr(e.length+1)},window.goatcounter.bind_events=function(){if(!document.querySelectorAll)return;var e=function(e){return function(){goatcounter.count({event:!0,path:e.dataset.goatcounterClick||e.name||e.id||"",title:e.dataset.goatcounterTitle||e.title||(e.innerHTML||"").substr(0,200)||"",referrer:e.dataset.goatcounterReferrer||e.dataset.goatcounterReferral||""})}};Array.prototype.slice.call(document.querySelectorAll("*[data-goatcounter-click]")).forEach(function(t){if(t.dataset.goatcounterBound)return;var n=e(t);t.addEventListener("click",n,!1),t.addEventListener("auxclick",n,!1),t.dataset.goatcounterBound="true"})},window.goatcounter.visit_count=function(n){c(function(){n=n||{},n.type=n.type||"html",n.append=n.append||"body",n.path=n.path||r(),n.attr=n.attr||{width:"200",height:n.no_branding?"60":"80"},n.attr.src=a()+"er/"+e(n.path)+"."+e(n.type)+"?",n.no_branding&&(n.attr.src+="&no_branding=1"),n.style&&(n.attr.src+="&style="+e(n.style)),n.start&&(n.attr.src+="&start="+e(n.start)),n.end&&(n.attr.src+="&end="+e(n.end));var s,o,c,i={png:"img",svg:"img",html:"iframe"}[n.type];if(!i)return t("visit_count: unknown type: "+n.type);n.type==="html"&&(n.attr.frameborder="0",n.attr.scrolling="no"),s=document.createElement(i);for(c in n.attr)s.setAttribute(c,n.attr[c]);if(o=document.querySelector(n.append),!o)return t("visit_count: append not found: "+n.append);o.appendChild(s)})},location.hash==="#toggle-goatcounter"&&(localStorage.getItem("skipgc")==="t"?(localStorage.removeItem("skipgc","t"),alert("GoatCounter tracking is now ENABLED in this browser.")):(localStorage.setItem("skipgc","t"),alert("GoatCounter tracking is now DISABLED in this browser until "+location+" is loaded again."))),goatcounter.no_onload||c(function(){if(!("visibilityState"in document)||document.visibilityState==="visible")goatcounter.count();else{var e=function(){if(document.visibilityState!=="visible")return;document.removeEventListener("visibilitychange",e),goatcounter.count()};document.addEventListener("visibilitychange",e)}goatcounter.no_events||goatcounter.bind_events()})})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>