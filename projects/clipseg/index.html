<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Segmentation using CLIP | pu239's page</title>
<meta name=keywords content="image segmentation,CLIP,zero-shot learning,CNNs"><meta name=description content="Zero-shot segmentation using large pre-trained language-image models like CLIP
In this project, we explored language-driven zero-shot semantic segmentation using large pre-trained language-vision classification models like CLIP.
We made changes in the vision branch of CLIP, which includes architectures like ResNets and Vision Transformers (ViT) to be replaced with other segmentation based architectures like PSPNet, DeepLab, DPT.
We have used PSPNet along with CLIP&rsquo;s text transformer (frozen). This gives us a good starting point for results. However, with just this, the segmentation maps are blobby and the boundaries are not well-defined, though the classes are in their correct approximate locations. This is because the image encoder is tied to the text encoder&rsquo;s embeddings (semantically) because of training. We can resolve this by adding PSPNet without removing the CLIP image encoder and using CLIP&rsquo;s maps as pseudo-labels for training our segmentation model.
We are testing out methods to improve this."><meta name=author content><link rel=canonical href=https://plutonium-239.github.io/projects/clipseg/><meta name=google-site-verification content="m8D5EaRXMmbmTIrarfEuOk3y54T12n_B7OD5D9k-fR4"><link crossorigin=anonymous href=/assets/css/stylesheet.8afda564a37916d73595609479ad9497581eda46cd27f7872ab630886ce7f9ed.css integrity="sha256-iv2lZKN5Ftc1lWCUea2Ul1ge2kbNJ/eHKrYwiGzn+e0=" rel="preload stylesheet" as=style><link rel=icon href=https://plutonium-239.github.io/bmth+av_round.png><link rel=icon type=image/png sizes=16x16 href=https://plutonium-239.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://plutonium-239.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://plutonium-239.github.io/apple-touch-icon.png><link rel=mask-icon href=https://plutonium-239.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://plutonium-239.github.io/projects/clipseg/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Segmentation using CLIP"><meta property="og:description" content="Zero-shot segmentation using large pre-trained language-image models like CLIP
In this project, we explored language-driven zero-shot semantic segmentation using large pre-trained language-vision classification models like CLIP.
We made changes in the vision branch of CLIP, which includes architectures like ResNets and Vision Transformers (ViT) to be replaced with other segmentation based architectures like PSPNet, DeepLab, DPT.
We have used PSPNet along with CLIP&rsquo;s text transformer (frozen). This gives us a good starting point for results. However, with just this, the segmentation maps are blobby and the boundaries are not well-defined, though the classes are in their correct approximate locations. This is because the image encoder is tied to the text encoder&rsquo;s embeddings (semantically) because of training. We can resolve this by adding PSPNet without removing the CLIP image encoder and using CLIP&rsquo;s maps as pseudo-labels for training our segmentation model.
We are testing out methods to improve this."><meta property="og:type" content="article"><meta property="og:url" content="https://plutonium-239.github.io/projects/clipseg/"><meta property="og:image" content="https://plutonium-239.github.io/projects/clipseg/clip_seg_overview.png"><meta property="article:section" content="projects"><meta property="article:published_time" content="2022-10-17T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-17T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://plutonium-239.github.io/projects/clipseg/clip_seg_overview.png"><meta name=twitter:title content="Segmentation using CLIP"><meta name=twitter:description content="Zero-shot segmentation using large pre-trained language-image models like CLIP
In this project, we explored language-driven zero-shot semantic segmentation using large pre-trained language-vision classification models like CLIP.
We made changes in the vision branch of CLIP, which includes architectures like ResNets and Vision Transformers (ViT) to be replaced with other segmentation based architectures like PSPNet, DeepLab, DPT.
We have used PSPNet along with CLIP&rsquo;s text transformer (frozen). This gives us a good starting point for results. However, with just this, the segmentation maps are blobby and the boundaries are not well-defined, though the classes are in their correct approximate locations. This is because the image encoder is tied to the text encoder&rsquo;s embeddings (semantically) because of training. We can resolve this by adding PSPNet without removing the CLIP image encoder and using CLIP&rsquo;s maps as pseudo-labels for training our segmentation model.
We are testing out methods to improve this."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://plutonium-239.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Segmentation using CLIP","item":"https://plutonium-239.github.io/projects/clipseg/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Segmentation using CLIP","name":"Segmentation using CLIP","description":"Zero-shot segmentation using large pre-trained language-image models like CLIP In this project, we explored language-driven zero-shot semantic segmentation using large pre-trained language-vision classification models like CLIP.\nWe made changes in the vision branch of CLIP, which includes architectures like ResNets and Vision Transformers (ViT) to be replaced with other segmentation based architectures like PSPNet, DeepLab, DPT.\nWe have used PSPNet along with CLIP\u0026rsquo;s text transformer (frozen). This gives us a good starting point for results. However, with just this, the segmentation maps are blobby and the boundaries are not well-defined, though the classes are in their correct approximate locations. This is because the image encoder is tied to the text encoder\u0026rsquo;s embeddings (semantically) because of training. We can resolve this by adding PSPNet without removing the CLIP image encoder and using CLIP\u0026rsquo;s maps as pseudo-labels for training our segmentation model. We are testing out methods to improve this.\n","keywords":["image segmentation","CLIP","zero-shot learning","CNNs"],"articleBody":"Zero-shot segmentation using large pre-trained language-image models like CLIP In this project, we explored language-driven zero-shot semantic segmentation using large pre-trained language-vision classification models like CLIP.\nWe made changes in the vision branch of CLIP, which includes architectures like ResNets and Vision Transformers (ViT) to be replaced with other segmentation based architectures like PSPNet, DeepLab, DPT.\nWe have used PSPNet along with CLIP’s text transformer (frozen). This gives us a good starting point for results. However, with just this, the segmentation maps are blobby and the boundaries are not well-defined, though the classes are in their correct approximate locations. This is because the image encoder is tied to the text encoder’s embeddings (semantically) because of training. We can resolve this by adding PSPNet without removing the CLIP image encoder and using CLIP’s maps as pseudo-labels for training our segmentation model. We are testing out methods to improve this.\nSummaries of some papers related to language-vision are available here: pdf, web-view The papers covered are:\nOpen Vocabulary Scene Parsing 1 CLIP (Learning Transferable Visual Models From Natural Language Supervision)2 LSeg (Language-driven Semantic Segmentation)3 RegionCLIP: Region-based Language-Image Pretraining4 Open-Set Recognition: a Good Closed-Set Classifier is All You Need?5 MaskCLIP (Extract Free Dense Labels from CLIP)6 References Zhao, Hang et al. “Open Vocabulary Scene Parsing.” International Conference on Computer Vision (ICCV). 2017. ↩︎\nRadford, Alec, et al. “Learning Transferable Visual Models From Natural Language Supervision.” ArXiv, 2021, /abs/2103.00020. ↩︎\nBoyi Li, et al. “Language-driven Semantic Segmentation.” International Conference on Learning Representations. 2022. ↩︎\nZhong, Yiwu et al. “Regionclip: Region-based language-image pretraining.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. ↩︎\nSagar Vaze, et al. “Open-Set Recognition: a Good Closed-Set Classifier is All You Need?.” International Conference on Learning Representations. 2022. ↩︎\nZhou, Chong et al. “Extract Free Dense Labels from CLIP.” European Conference on Computer Vision (ECCV). 2022. ↩︎\n","wordCount":"304","inLanguage":"en","image":"https://plutonium-239.github.io/projects/clipseg/clip_seg_overview.png","datePublished":"2022-10-17T00:00:00Z","dateModified":"2022-10-17T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://plutonium-239.github.io/projects/clipseg/"},"publisher":{"@type":"Organization","name":"pu239's page","logo":{"@type":"ImageObject","url":"https://plutonium-239.github.io/bmth+av_round.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://plutonium-239.github.io/ accesskey=h title="pu239's page (Alt + H)"><img src=https://plutonium-239.github.io/bmth+av.png alt aria-label=logo height=40>pu239's page</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://plutonium-239.github.io/about/ title="about me"><span>about me</span></a></li><li><a href=https://plutonium-239.github.io/publications/ title=publications><span>publications</span></a></li><li><a href=https://plutonium-239.github.io/projects/ title=projects><span class=active>projects</span></a></li><li><a href=https://plutonium-239.github.io/other_stuff/ title="other stuff"><span>other stuff</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><span class=breadcrumbs-lang-badge><div class=breadcrumbs><a href=https://plutonium-239.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://plutonium-239.github.io/projects/></a></div></span><h1 class="post-title entry-hint-parent">Segmentation using CLIP</h1><div class=post-meta><span title='2022-10-17 00:00:00 +0000 UTC'>October 17, 2022</span><ul class=post-tags><li><a href=https://plutonium-239.github.io/tags/image-segmentation/>Image Segmentation</a></li><li><a href=https://plutonium-239.github.io/tags/clip/>CLIP</a></li><li><a href=https://plutonium-239.github.io/tags/zero-shot-learning/>Zero-Shot Learning</a></li><li><a href=https://plutonium-239.github.io/tags/cnns/>CNNs</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://plutonium-239.github.io/clip_seg_overview.png alt=Overview><p>Overview</p></figure><div class=post-content><h2 id=zero-shot-segmentation-using-large-pre-trained-language-image-models-like-clip>Zero-shot segmentation using large pre-trained language-image models like CLIP<a hidden class=anchor aria-hidden=true href=#zero-shot-segmentation-using-large-pre-trained-language-image-models-like-clip>#</a></h2><p>In this project, we explored language-driven zero-shot semantic segmentation using large pre-trained language-vision classification models like CLIP.</p><p>We made changes in the vision branch of CLIP, which includes architectures like ResNets and Vision Transformers (ViT) to be replaced with other segmentation based architectures like PSPNet, DeepLab, DPT.</p><p>We have used PSPNet along with CLIP&rsquo;s text transformer (frozen). This gives us a good starting point for results. However, with just this, the segmentation maps are blobby and the boundaries are not well-defined, though the classes are in their correct approximate locations. This is because the image encoder is tied to the text encoder&rsquo;s embeddings (semantically) because of training. We can resolve this by adding PSPNet without removing the CLIP image encoder and using CLIP&rsquo;s maps as pseudo-labels for training our segmentation model.
We are testing out methods to improve this.</p><p>Summaries of some papers related to language-vision are available here: <a href=paper-summaries.pdf>pdf</a>, <a href=https://hackmd.io/@pu239/language-based-seg>web-view</a>
The papers covered are:</p><ol><li>Open Vocabulary Scene Parsing <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></li><li>CLIP (Learning Transferable Visual Models From Natural Language Supervision)<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></li><li>LSeg (Language-driven Semantic Segmentation)<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></li><li>RegionCLIP: Region-based Language-Image Pretraining<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></li><li>Open-Set Recognition: a Good Closed-Set Classifier is All You Need?<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></li><li>MaskCLIP (Extract Free Dense Labels from CLIP)<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></li></ol><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhao_Open_Vocabulary_Scene_ICCV_2017_paper.pdf>Zhao, Hang et al. &ldquo;Open Vocabulary Scene Parsing.&rdquo; <em>International Conference on Computer Vision (ICCV)</em>. 2017.</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://openai.com/research/clip>Radford, Alec, et al. &ldquo;Learning Transferable Visual Models From Natural Language Supervision.&rdquo; <em>ArXiv</em>, 2021, /abs/2103.00020.</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href="https://openreview.net/forum?id=RriDjddCLN">Boyi Li, et al. &ldquo;Language-driven Semantic Segmentation.&rdquo; <em>International Conference on Learning Representations</em>. 2022.</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf>Zhong, Yiwu et al. &ldquo;Regionclip: Region-based language-image pretraining.&rdquo; <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2022.</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href="https://openreview.net/forum?id=5hLP5JY9S2d">Sagar Vaze, et al. &ldquo;Open-Set Recognition: a Good Closed-Set Classifier is All You Need?.&rdquo; <em>International Conference on Learning Representations</em>. 2022.</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://arxiv.org/abs/2112.01071>Zhou, Chong et al. &ldquo;Extract Free Dense Labels from CLIP.&rdquo; <em>European Conference on Computer Vision (ECCV)</em>. 2022.</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://plutonium-239.github.io/tags/image-segmentation/>Image Segmentation</a></li><li><a href=https://plutonium-239.github.io/tags/clip/>CLIP</a></li><li><a href=https://plutonium-239.github.io/tags/zero-shot-learning/>Zero-Shot Learning</a></li><li><a href=https://plutonium-239.github.io/tags/cnns/>CNNs</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://plutonium-239.github.io/>pu239's page</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/lightbox.js></script><link rel=stylesheet href=/lightbox.css><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>